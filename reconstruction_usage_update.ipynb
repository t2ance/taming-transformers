{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "reconstruction_usage_update.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e3fb29732994a9f982a7c33a29e8562": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ad1eef547e784583af9372b52f308920",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_1b54f204b26f45de9313d01b1e08e2a9",
       "IPY_MODEL_e26c627b89ea43dda225c75aec2ace89"
      ]
     },
     "model_module_version": "1.5.0"
    },
    "ad1eef547e784583af9372b52f308920": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "1b54f204b26f45de9313d01b1e08e2a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_6f3a018a2abb4db889180b23065bdd31",
      "_dom_classes": [],
      "description": "100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 553433881,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 553433881,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6716348a16c146b59a231dec0b59c714"
     },
     "model_module_version": "1.5.0"
    },
    "e26c627b89ea43dda225c75aec2ace89": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_a27d6eee576c4c158bc97c531374f855",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 528M/528M [00:05&lt;00:00, 97.3MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_27bfdf88e3c244cea87e2b9c932678d2"
     },
     "model_module_version": "1.5.0"
    },
    "6f3a018a2abb4db889180b23065bdd31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "6716348a16c146b59a231dec0b59c714": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "a27d6eee576c4c158bc97c531374f855": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "27bfdf88e3c244cea87e2b9c932678d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbYB4cfoxTXz"
   },
   "source": [
    "# Taming Transformers\n",
    "## Reconstruction Capabilities of VQGAN (Colab Notebook)\n",
    "\n",
    "This notebook provides code to (visually) analyze the first stage models used to generate images as in [Taming Transformers for High-Resolution Image Synthesis](https://github.com/CompVis/taming-transformers)\n",
    "and a comparison to the first stage model used in [DALL-E](https://openai.com/blog/dall-e/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RxdhDGtyJ4q"
   },
   "source": [
    "### Setup\n",
    "Clone the repository and download pretrained VQGANs: a small one with a codebook dimensionality $\\dim \\mathcal{Z} = 1024$\n",
    "and a larger one with $ \\dim \\mathcal{Z} = 16384$. Both perform *four* downsampling steps, e.g. an input image of\n",
    "size $256 \\times 256$ will be mapped to a latent code of size $16 \\times 16$.\n",
    "Additionally, also load a model which only uses three downsampling steps, such that the latent code will have size $32 \\times 32$. The increased capacity of the representation helps to produce higher quality reconstructions, but this makes training a transformer model with full attention much more expensive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npUIoYn85PU9",
    "outputId": "7bfcd8e8-28e5-4cde-ca59-daaef3204888",
    "ExecuteTime": {
     "end_time": "2025-01-01T09:18:27.767978800Z",
     "start_time": "2025-01-01T09:18:27.704275200Z"
    }
   },
   "source": [
    "# !git clone https://github.com/CompVis/taming-transformers\n",
    "# %cd taming-transformers"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/taming/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T09:18:27.777325600Z",
     "start_time": "2025-01-01T09:18:27.737019400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S4G0FaWPx2bE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "98f8c88a-75f1-4897-c16f-8570c18f0c4a",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-01-01T09:18:27.742204900Z"
    }
   },
   "source": [
    "# download a VQGAN with f=16 (16x compression per spatial dimension) and with a codebook with 1024 entries\n",
    "!mkdir -p logs/vqgan_imagenet_f16_1024/checkpoints\n",
    "!mkdir -p logs/vqgan_imagenet_f16_1024/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' -O 'logs/vqgan_imagenet_f16_1024/configs/model.yaml'\n",
    "\n",
    "# download a VQGAN with f=16 (16x compression per spatial dimension) and with a larger codebook (16384 entries)\n",
    "!mkdir -p logs/vqgan_imagenet_f16_16384/checkpoints\n",
    "!mkdir -p logs/vqgan_imagenet_f16_16384/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'logs/vqgan_imagenet_f16_16384/configs/model.yaml'\n",
    "\n",
    "# download a VQGAN with f=8 (8x compression per spatial dimension) and a larger codebook-size with 8192 entries\n",
    "!mkdir -p logs/vqgan_gumbel_f8/checkpoints\n",
    "!mkdir -p logs/vqgan_gumbel_f8/configs\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1' -O 'logs/vqgan_gumbel_f8/checkpoints/last.ckpt'\n",
    "!wget 'https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1' -O 'logs/vqgan_gumbel_f8/configs/model.yaml'"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-01 09:18:31--  https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1\r\n",
      "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\r\n",
      "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://heibox.uni-heidelberg.de/seafhttp/files/7366a44d-b9ab-4800-8af5-79a5193bcab0/last.ckpt [following]\r\n",
      "--2025-01-01 09:18:32--  https://heibox.uni-heidelberg.de/seafhttp/files/7366a44d-b9ab-4800-8af5-79a5193bcab0/last.ckpt\r\n",
      "Reusing existing connection to heibox.uni-heidelberg.de:443.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 957954257 (914M) [application/octet-stream]\r\n",
      "Saving to: â€˜logs/vqgan_imagenet_f16_1024/checkpoints/last.ckptâ€™\r\n",
      "\r\n",
      "                log  90%[=================>  ] 826.18M  14.9MB/s    eta 6s     "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygLMU_C06tNG"
   },
   "source": [
    "Install minimal required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Pyzy3Fp60Bs",
    "is_executing": true
   },
   "source": [
    "%%capture\n",
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops>=0.3.0\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!conda env list"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip list"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utfHGXoWM79i"
   },
   "source": [
    "\n",
    "Define some loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gfIM7FvJ656P",
    "is_executing": true
   },
   "source": [
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if display:\n",
    "        print(yaml.dump(OmegaConf.to_container(config)))\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "    if is_gumbel:\n",
    "        model = GumbelVQ(**config.model.params)\n",
    "    else:\n",
    "        model = VQModel(**config.model.params)\n",
    "    if ckpt_path is not None:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "    x = 2. * x - 1.\n",
    "    return x\n",
    "\n",
    "\n",
    "def custom_to_pil(x):\n",
    "    x = x.detach().cpu()\n",
    "    x = torch.clamp(x, -1., 1.)\n",
    "    x = (x + 1.) / 2.\n",
    "    x = x.permute(1, 2, 0).numpy()\n",
    "    x = (255 * x).astype(np.uint8)\n",
    "    x = Image.fromarray(x)\n",
    "    if not x.mode == \"RGB\":\n",
    "        x = x.convert(\"RGB\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "    # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "    z, _, [_, _, indices] = model.encode(x)\n",
    "    print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "    xrec = model.decode(z)\n",
    "    return xrec"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3MUo8a8Pd_T"
   },
   "source": [
    "## Load the VQGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbPSwvOcOknb"
   },
   "source": [
    "First, load (and optionally display) the model configs. Then, load the VQGAN models.\n",
    "Start with the f=16 models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239,
     "referenced_widgets": [
      "1e3fb29732994a9f982a7c33a29e8562",
      "ad1eef547e784583af9372b52f308920",
      "1b54f204b26f45de9313d01b1e08e2a9",
      "e26c627b89ea43dda225c75aec2ace89",
      "6f3a018a2abb4db889180b23065bdd31",
      "6716348a16c146b59a231dec0b59c714",
      "a27d6eee576c4c158bc97c531374f855",
      "27bfdf88e3c244cea87e2b9c932678d2"
     ]
    },
    "id": "G7XERAfxNi3P",
    "outputId": "9b4bdeab-b208-4f0f-d4b3-52ed8b8a9ea0",
    "is_executing": true
   },
   "source": [
    "config1024 = load_config(\"logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n",
    "config16384 = load_config(\"logs/vqgan_imagenet_f16_16384/configs/model.yaml\", display=False)\n",
    "\n",
    "model1024 = load_vqgan(config1024, ckpt_path=\"logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n",
    "model16384 = load_vqgan(config16384, ckpt_path=\"logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juA0_jipeI8u"
   },
   "source": [
    "Also load a f8 model. This model trades compressive power for higher reconstruction fidelity.\n",
    "For example, an input image of size $256 \\times 256$ will be encoded to a representation of size $32\\times 32$. Due to the\n",
    "quadratic complexity of the attention mechanism, this makes downstream\n",
    "autogressive training of a full-attention transformer model much more expensive (by a factor 16), as the unrolled sequence now\n",
    "has a length of $32\\cdot 32 = 1024$ (compare this to a f16-VQGAN which gives a representation of size $16\\cdot 16 = 256$)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zXAufbe7fmgu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2deb1b05-6aaf-4ca4-a5f4-9a9d04571d5c",
    "is_executing": true
   },
   "source": [
    "config32x32 = load_config(\"logs/vqgan_gumbel_f8/configs/model.yaml\", display=False)\n",
    "model32x32 = load_vqgan(config32x32, ckpt_path=\"logs/vqgan_gumbel_f8/checkpoints/last.ckpt\", is_gumbel=True).to(DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4E8yZNjXH8J"
   },
   "source": [
    "## DALL-E joins the party\n",
    "Code reproduced from the official notebook available at https://github.com/openai/DALL-E/blob/master/notebooks/usage.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WTTJgu-mXG8_",
    "is_executing": true
   },
   "source": [
    "%pip install git+https://github.com/openai/DALL-E.git &> /dev/null"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ikPq5rvxXucQ",
    "is_executing": true
   },
   "source": [
    "import io\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display\n",
    "\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf\", 22)\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "\n",
    "def preprocess(img, target_image_size=256, map_dalle=True):\n",
    "    s = min(img.size)\n",
    "\n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "\n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    if map_dalle:\n",
    "        img = map_pixels(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def reconstruct_with_dalle(x, encoder, decoder, do_preprocess=False):\n",
    "    # takes in tensor (or optionally, a PIL image) and returns a PIL image\n",
    "    if do_preprocess:\n",
    "        x = preprocess(x)\n",
    "    z_logits = encoder(x)\n",
    "    z = torch.argmax(z_logits, axis=1)\n",
    "\n",
    "    print(f\"DALL-E: latent shape: {z.shape}\")\n",
    "    z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    x_stats = decoder(z).float()\n",
    "    x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n",
    "    x_rec = T.ToPILImage(mode='RGB')(x_rec[0])\n",
    "\n",
    "    return x_rec\n",
    "\n",
    "\n",
    "def stack_reconstructions(input, x0, x1, x2, x3, titles=[]):\n",
    "    assert input.size == x1.size == x2.size == x3.size\n",
    "    w, h = input.size[0], input.size[1]\n",
    "    img = Image.new(\"RGB\", (5 * w, h))\n",
    "    img.paste(input, (0, 0))\n",
    "    img.paste(x0, (1 * w, 0))\n",
    "    img.paste(x1, (2 * w, 0))\n",
    "    img.paste(x2, (3 * w, 0))\n",
    "    img.paste(x3, (4 * w, 0))\n",
    "    for i, title in enumerate(titles):\n",
    "        ImageDraw.Draw(img).text((i * w, 0), f'{title}', (255, 255, 255), font=font)  # coordinates, text, color, font\n",
    "    return img"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A559hbAEYQ6v"
   },
   "source": [
    "Load the provided encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zK6EIoazYL1k",
    "is_executing": true
   },
   "source": [
    "# For faster load times, download these files locally and use the local paths instead.\n",
    "encoder_dalle = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", DEVICE)\n",
    "decoder_dalle = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ui9910mmvFy"
   },
   "source": [
    "## Reconstruct some images\n",
    "\n",
    "Define the reconstruction pipeline and stack the reconstructions for a direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3aNc2aEEs0BY",
    "is_executing": true
   },
   "source": [
    "titles = [\"Input\", \"DALL-E dVAE (f8, 8192)\", \"VQGAN (f8, 8192)\", \"VQGAN (f16, 16384)\", \"VQGAN (f16, 1024)\"]\n",
    "\n",
    "\n",
    "def reconstruction_pipeline(url, size=320):\n",
    "    x_dalle = preprocess(download_image(url), target_image_size=size, map_dalle=True)\n",
    "    x_vqgan = preprocess(download_image(url), target_image_size=size, map_dalle=False)\n",
    "    x_dalle = x_dalle.to(DEVICE)\n",
    "    x_vqgan = x_vqgan.to(DEVICE)\n",
    "\n",
    "    print(f\"input is of size: {x_vqgan.shape}\")\n",
    "    x0 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model32x32)\n",
    "    x1 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model16384)\n",
    "    x2 = reconstruct_with_vqgan(preprocess_vqgan(x_vqgan), model1024)\n",
    "    x3 = reconstruct_with_dalle(x_dalle, encoder_dalle, decoder_dalle)\n",
    "    img = stack_reconstructions(custom_to_pil(preprocess_vqgan(x_vqgan[0])), x3,\n",
    "                                custom_to_pil(x0[0]), custom_to_pil(x1[0]),\n",
    "                                custom_to_pil(x2[0]), titles=titles)\n",
    "    return img"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmUzRLOHlJ8i"
   },
   "source": [
    "Let's reconstruct some images from the [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "zwXrRtgMA_B8",
    "outputId": "0e768e00-d872-4092-efbf-2cfc816489e2",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1', size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH0kKuwoqBoN"
   },
   "source": [
    "Especially at regions like the squirrel's fur and tail, the VQGANs produce plausible textures whereas the first stage of DALL-E produces overly smooth regions despite using four times more codes. On the other hand, using fewer codes means that the VQGAN cannot reproduce every detail of its input but instead hallucinates parts of it. In particular, the VQGAN (1024) has difficulties reconstructing the foot, which can be remedied to some degree by the bigger codebook size of VQGAN (16384)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "-f4jH-puBCP0",
    "outputId": "ac3062a6-c00f-40f4-f9f3-99eaadc4987f",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/6f12b330eb564d288d76/?dl=1', size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "OyEdKjuAhMMM",
    "outputId": "7213fad0-7391-4cb0-e444-5a14dbf08b54",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/8555a959b0a5423cbfd1/?dl=1', size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "DZCMKe-Ptapi",
    "outputId": "cfd4af2b-654f-405d-ca1a-69949800eada",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(url='https://heibox.uni-heidelberg.de/f/be6f4ff34e1544109563/?dl=1', size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivbDgn-atJa7"
   },
   "source": [
    "Faces are particularly difficult for the VQGAN to get right and the reconstructions of DALL-E's first stage appear more presentable. However, it should also be noted that the latter has been trained on a dataset which is roughly 400 times larger than the dataset (ImageNet) that the VQGAN was trained on. Thus, training the VQGAN on a larger dataset, or fine-tuning it on a dataset containing more faces, could improve the perceptual quality of reconstructed faces (and VQGANs trained on face datasets only do not show this problem)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "vlY69zv_4P9r",
    "outputId": "1b1bf3d3-2747-4e4b-9e71-cc28d0e770e7",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(\"https://heibox.uni-heidelberg.de/f/e41f5053cbd34f11a8d5/?dl=1\", size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9ORpPaLTBNP"
   },
   "source": [
    "And finally, the penguin."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "XlSllawMSwfi",
    "outputId": "40b5c06c-2727-4453-c0d4-329f673963ee",
    "is_executing": true
   },
   "source": [
    "reconstruction_pipeline(url='https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg', size=384)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQKgzqrywDKa"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "These examples show that the use of an adversarial loss applied in a patch-wise manner does indeed help to produce reconstructions that favor *realism* over a perfect reconstruction (but may cause \"deletion\" of certain objects, such as the pine cone in the 4th example). Furthermore, the adversarial training enables very agressive downsampling: Given an image of size $256 \\times 256$, the VQGAN produces a sequence of length $16 \\cdot 16 = 256$ (vs. $32 \\cdot 32 = 1024$ for DALL-E). This supports downstream tasks such as training attention-based models on the latent space, which, in their basic form, scale quadratically with sequence length. Thus, training the same transformer on top of the codes produced by the VQGAN is roughly 16 times faster than training it on top of DALL-E's first stage.\n",
    "\n",
    "We also observe that a more realistic and faithful reconstruction can be achieved if $\\vert \\mathcal{Z} \\vert$ is increased or the compression rate is decreased (f8-model). Note, however, that a lower latent dimensionality can also help a downstream autoregressive likelihood model (such as our transformer) to generate more globally coherent structures and descreases training cost.\n",
    "\n",
    "\n",
    "One way to quantify the amount of \"realism\" captured by these models is to compute FID scores of reconstructed images w.r.t. the inputs (R-FIDs). The following table shows R-FIDs when reconstructing the validation split of the ImageNet dataset ($256 \\times 256$ px images). Additionally, we also evaluate the perceptual similarity between inputs and reconstructions with the [LPIPS](https://richzhang.github.io/PerceptualSimilarity/) metric and structural similarity through PSNR and SSIM.\n",
    "\n",
    "\n",
    "|   | VQGAN f16 (16384) |  VQGAN f16 (1024)  |  DALL-E f8 (8192)| VQGAN f8 (8192)|\n",
    "|---| :---:| :---: | :---: | :---: |\n",
    "| R-FID \t$\\downarrow$ | 4.98 | 7.94 | 32.01 | 1.49 |\n",
    "| LPIPS \t$\\downarrow$ | 1.83 +/- 0.42 | 1.98 +/- 0.43 | 1.95 +/- 0.51 | 1.17 +/- 0.34 |\n",
    "| PSNR \t$\\uparrow$ | 19.9 +/- 3.4 | 19.4 +/- 3.3 | 22.8 +/- 2.1 | 22.2 +/- 3.8 |\n",
    "| SSIM \t$\\uparrow$ | 0.51 +/- 0.18 | 0.50 +/- 0.18 | 0.73 +/- 0.13 | 0.65 +/- 0.16 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tFCBCUs6VgX"
   },
   "source": [
    "Finally, note that these models can be used in a fully convolutional fashion: For an input of size $(h, w)$, the corresponding latent representation is always $(h/2^m, w/2^m)$, with $m=4$ for the VQGANs presented here and $m=3$ for the autoencoder of DALL-E."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "MJowCywz-sN6",
    "outputId": "85a3f757-552d-4b4a-c012-754e48e2e407",
    "is_executing": true
   },
   "source": [
    "display(reconstruction_pipeline(\"https://heibox.uni-heidelberg.de/f/5cfd15de5d104d6fbce4/?dl=1\", size=320))\n",
    "display(reconstruction_pipeline(\"https://heibox.uni-heidelberg.de/f/5cfd15de5d104d6fbce4/?dl=1\", size=512))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G076qevn-PmY",
    "is_executing": true
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
